{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1f118101669c4795ae8c0e56f1c7fa14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1030c057ea7f419a96081d8912cbbc73","IPY_MODEL_8dd0ca5723ce40c094048f466ae71264","IPY_MODEL_523ffa9dc6fb489e897e796cf402ffb9"],"layout":"IPY_MODEL_e159b3ec68ff4aa2b57a12491fe5befe"}},"1030c057ea7f419a96081d8912cbbc73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae5be16f0efa4ba7a9d31945bc4a8f27","placeholder":"​","style":"IPY_MODEL_6ee1e468ba7943889ef6d9e3d83df080","value":"Downloading builder script: 100%"}},"8dd0ca5723ce40c094048f466ae71264":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8743d079e514edaaafc999360d8b677","max":8643,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fac53eeb7d74431396525900e4bfebbc","value":8643}},"523ffa9dc6fb489e897e796cf402ffb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a7e15aa62274a6da76de4e8d7fc8aa8","placeholder":"​","style":"IPY_MODEL_1ef747a4912d444d89e15bf048217203","value":" 8.64k/8.64k [00:00&lt;00:00, 330kB/s]"}},"e159b3ec68ff4aa2b57a12491fe5befe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae5be16f0efa4ba7a9d31945bc4a8f27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ee1e468ba7943889ef6d9e3d83df080":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8743d079e514edaaafc999360d8b677":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac53eeb7d74431396525900e4bfebbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a7e15aa62274a6da76de4e8d7fc8aa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ef747a4912d444d89e15bf048217203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1f24264b0e14585ac9d536a5504ab3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95195a7eb56e45de88984191b0430129","IPY_MODEL_0e98f3b5915844fe9766bec29d507fa3","IPY_MODEL_f32f292355c44feaac328b5d5b21d375"],"layout":"IPY_MODEL_cd527f73adf547df8ee2e259d838a4b4"}},"95195a7eb56e45de88984191b0430129":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ad2a42b9152498c9617225383ddc1a7","placeholder":"​","style":"IPY_MODEL_f1d89c301063459baf3390ffa6653d14","value":"Downloading readme: 100%"}},"0e98f3b5915844fe9766bec29d507fa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f69f0a004d04ba7be6ee8f521784772","max":11863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d871a4623c7247c187603ea32c4ed9f6","value":11863}},"f32f292355c44feaac328b5d5b21d375":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f0dc3aa956b4d46834f27e4c77d5342","placeholder":"​","style":"IPY_MODEL_993d836e97d841a08de8cda83b97c784","value":" 11.9k/11.9k [00:00&lt;00:00, 623kB/s]"}},"cd527f73adf547df8ee2e259d838a4b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ad2a42b9152498c9617225383ddc1a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d89c301063459baf3390ffa6653d14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f69f0a004d04ba7be6ee8f521784772":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d871a4623c7247c187603ea32c4ed9f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f0dc3aa956b4d46834f27e4c77d5342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993d836e97d841a08de8cda83b97c784":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"361084c2af9b4f11be6384a96b66dd08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ba6a23049d34011b868ba9d74712d97","IPY_MODEL_c25aa7030a8d469ea1df97dc6c5c5c9e","IPY_MODEL_932e0e28a9854f65a1ce47afe56b03a8"],"layout":"IPY_MODEL_8f2bb2bc91d043dcbef5c60f8ab29748"}},"6ba6a23049d34011b868ba9d74712d97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8e1666b04c240e68847f13dd71d1984","placeholder":"​","style":"IPY_MODEL_40a5e917c97e44e7b736387891d09a23","value":"Downloading data: 100%"}},"c25aa7030a8d469ea1df97dc6c5c5c9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44ba2dff32cc4ab3954e9f00ab7c62c4","max":27147920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_733e4e4840ad44298c77897a0f9c986e","value":27147920}},"932e0e28a9854f65a1ce47afe56b03a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5698a85a2a94a359e7d2955ef540c90","placeholder":"​","style":"IPY_MODEL_af90885027614576b2c93b7cc44c74f7","value":" 27.1M/27.1M [00:01&lt;00:00, 22.9MB/s]"}},"8f2bb2bc91d043dcbef5c60f8ab29748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8e1666b04c240e68847f13dd71d1984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40a5e917c97e44e7b736387891d09a23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44ba2dff32cc4ab3954e9f00ab7c62c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"733e4e4840ad44298c77897a0f9c986e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5698a85a2a94a359e7d2955ef540c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af90885027614576b2c93b7cc44c74f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ac9ca9c7d244ae9b36872e52f355a5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_177fa266890940a987607b76f9d7103c","IPY_MODEL_1684f55424c249f1bd366ae370445aae","IPY_MODEL_5dff8da469ec45eeb6f37fae2b3e8813"],"layout":"IPY_MODEL_2d3c9b153efc42c08618c739ef6c240e"}},"177fa266890940a987607b76f9d7103c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_012994bab4a3491dbd4c6abc073b9b42","placeholder":"​","style":"IPY_MODEL_2a8f11b5a155479a9e7b7ce3b8346185","value":"Generating train split: 100%"}},"1684f55424c249f1bd366ae370445aae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42ea5ac7da9425eb8c75f43fb3c7ca0","max":1020868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d49ea3900afd470baa0f877a0136b70e","value":1020868}},"5dff8da469ec45eeb6f37fae2b3e8813":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c107f062c4b4d2b8eab89c98f579f04","placeholder":"​","style":"IPY_MODEL_16e54011c3de4170ad0968b1433a5f3d","value":" 1020868/1020868 [01:45&lt;00:00, 13416.33 examples/s]"}},"2d3c9b153efc42c08618c739ef6c240e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"012994bab4a3491dbd4c6abc073b9b42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a8f11b5a155479a9e7b7ce3b8346185":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a42ea5ac7da9425eb8c75f43fb3c7ca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d49ea3900afd470baa0f877a0136b70e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c107f062c4b4d2b8eab89c98f579f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16e54011c3de4170ad0968b1433a5f3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"594f11e87a834d47a8075fbea4a30604":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f5ccd640b8f4631a88f99534b450797","IPY_MODEL_dcdc83175b9a4e39b71f54d37a18c58d","IPY_MODEL_04551dacf8de4bd0b27822c0aba8e649"],"layout":"IPY_MODEL_e533c9aaee554b5ebb516bdaca8e9456"}},"9f5ccd640b8f4631a88f99534b450797":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c577ccf270204598abec83bb751f642e","placeholder":"​","style":"IPY_MODEL_6b4c20e56a7146eba95a481fb6ea92d8","value":"Filter: 100%"}},"dcdc83175b9a4e39b71f54d37a18c58d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3447f58ed0c416680c98afee5aae38e","max":1020868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32dc48a9e5564a62b0d3d98fef867236","value":1020868}},"04551dacf8de4bd0b27822c0aba8e649":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e202ad817b64903868c588473a09ff8","placeholder":"​","style":"IPY_MODEL_25a7cfcdabc742deb4c5b1bbc5aa80de","value":" 1020868/1020868 [00:12&lt;00:00, 91467.88 examples/s]"}},"e533c9aaee554b5ebb516bdaca8e9456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c577ccf270204598abec83bb751f642e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4c20e56a7146eba95a481fb6ea92d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3447f58ed0c416680c98afee5aae38e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32dc48a9e5564a62b0d3d98fef867236":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e202ad817b64903868c588473a09ff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a7cfcdabc742deb4c5b1bbc5aa80de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Reconstruction","metadata":{"id":"ElNaMbLnRdHR"}},{"cell_type":"markdown","source":"The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n\nThe otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n\n\nCONSTRAINTS:\n* No pretrained model can be used.\n* The neural network models should have less the 20M parameters.\n* No postprocessing should be done (e.g. no beamsearch)\n* You cannot use additional training data.\n\n\nBONUS PARAMETERS:\n\nA bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters.","metadata":{"id":"oXr4iGUGRms8"}},{"cell_type":"code","source":"#!pip install datasets\n#!pip install --upgrade keras","metadata":{"id":"YnGsq3WSamPP","outputId":"2b023460-e954-44de-a6c4-e8aa19754671","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-06-12T04:43:08.208455Z","iopub.execute_input":"2024-06-12T04:43:08.209410Z","iopub.status.idle":"2024-06-12T04:43:08.214313Z","shell.execute_reply.started":"2024-06-12T04:43:08.209365Z","shell.execute_reply":"2024-06-12T04:43:08.213288Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras import ops\nfrom keras import layers\nfrom keras.layers import Embedding\n\nfrom datasets import load_dataset\nimport string\nimport re\n\nimport numpy as np\nimport math","metadata":{"id":"_WjtqA8TrHcS","execution":{"iopub.status.busy":"2024-06-12T04:43:08.216282Z","iopub.execute_input":"2024-06-12T04:43:08.216592Z","iopub.status.idle":"2024-06-12T04:43:08.226025Z","shell.execute_reply.started":"2024-06-12T04:43:08.216567Z","shell.execute_reply":"2024-06-12T04:43:08.225207Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"### Download the dataset and declare variables","metadata":{"id":"807Wk-ir_bDU"}},{"cell_type":"code","source":"VOCAB_SIZE = 10000\nSEQ_LEN = 28\nBATCH_SIZE = 256","metadata":{"id":"j04xaRci9wO_","execution":{"iopub.status.busy":"2024-06-12T04:43:08.227066Z","iopub.execute_input":"2024-06-12T04:43:08.227331Z","iopub.status.idle":"2024-06-12T04:43:08.237292Z","shell.execute_reply.started":"2024-06-12T04:43:08.227308Z","shell.execute_reply":"2024-06-12T04:43:08.236385Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"ds = load_dataset('generics_kb', trust_remote_code=True)['train']\nds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8 )","metadata":{"id":"sihcZOrp9wPB","outputId":"fef017d3-aa0b-48f6-f58d-529455634d1a","colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["1f118101669c4795ae8c0e56f1c7fa14","1030c057ea7f419a96081d8912cbbc73","8dd0ca5723ce40c094048f466ae71264","523ffa9dc6fb489e897e796cf402ffb9","e159b3ec68ff4aa2b57a12491fe5befe","ae5be16f0efa4ba7a9d31945bc4a8f27","6ee1e468ba7943889ef6d9e3d83df080","c8743d079e514edaaafc999360d8b677","fac53eeb7d74431396525900e4bfebbc","7a7e15aa62274a6da76de4e8d7fc8aa8","1ef747a4912d444d89e15bf048217203","e1f24264b0e14585ac9d536a5504ab3a","95195a7eb56e45de88984191b0430129","0e98f3b5915844fe9766bec29d507fa3","f32f292355c44feaac328b5d5b21d375","cd527f73adf547df8ee2e259d838a4b4","1ad2a42b9152498c9617225383ddc1a7","f1d89c301063459baf3390ffa6653d14","0f69f0a004d04ba7be6ee8f521784772","d871a4623c7247c187603ea32c4ed9f6","6f0dc3aa956b4d46834f27e4c77d5342","993d836e97d841a08de8cda83b97c784","361084c2af9b4f11be6384a96b66dd08","6ba6a23049d34011b868ba9d74712d97","c25aa7030a8d469ea1df97dc6c5c5c9e","932e0e28a9854f65a1ce47afe56b03a8","8f2bb2bc91d043dcbef5c60f8ab29748","f8e1666b04c240e68847f13dd71d1984","40a5e917c97e44e7b736387891d09a23","44ba2dff32cc4ab3954e9f00ab7c62c4","733e4e4840ad44298c77897a0f9c986e","f5698a85a2a94a359e7d2955ef540c90","af90885027614576b2c93b7cc44c74f7","3ac9ca9c7d244ae9b36872e52f355a5b","177fa266890940a987607b76f9d7103c","1684f55424c249f1bd366ae370445aae","5dff8da469ec45eeb6f37fae2b3e8813","2d3c9b153efc42c08618c739ef6c240e","012994bab4a3491dbd4c6abc073b9b42","2a8f11b5a155479a9e7b7ce3b8346185","a42ea5ac7da9425eb8c75f43fb3c7ca0","d49ea3900afd470baa0f877a0136b70e","4c107f062c4b4d2b8eab89c98f579f04","16e54011c3de4170ad0968b1433a5f3d","594f11e87a834d47a8075fbea4a30604","9f5ccd640b8f4631a88f99534b450797","dcdc83175b9a4e39b71f54d37a18c58d","04551dacf8de4bd0b27822c0aba8e649","e533c9aaee554b5ebb516bdaca8e9456","c577ccf270204598abec83bb751f642e","6b4c20e56a7146eba95a481fb6ea92d8","a3447f58ed0c416680c98afee5aae38e","32dc48a9e5564a62b0d3d98fef867236","4e202ad817b64903868c588473a09ff8","25a7cfcdabc742deb4c5b1bbc5aa80de"]},"execution":{"iopub.status.busy":"2024-06-12T04:43:08.238753Z","iopub.execute_input":"2024-06-12T04:43:08.239040Z","iopub.status.idle":"2024-06-12T04:43:09.168254Z","shell.execute_reply.started":"2024-06-12T04:43:08.239009Z","shell.execute_reply":"2024-06-12T04:43:09.167516Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"### Create the tokenizer and detokenizer\nDefine the tokens that are gonna be used by the tokenizer","metadata":{"id":"-57Vni-5cU99"}},{"cell_type":"code","source":"#Define a class that contains all the token that we are gonna need\nclass Tokens:\n    COMMA = '<comma>'\n    START = '<start>'\n    END = '<end>'","metadata":{"id":"xwsxyqqN9wPA","execution":{"iopub.status.busy":"2024-06-12T04:43:09.170814Z","iopub.execute_input":"2024-06-12T04:43:09.171108Z","iopub.status.idle":"2024-06-12T04:43:09.175226Z","shell.execute_reply.started":"2024-06-12T04:43:09.171081Z","shell.execute_reply":"2024-06-12T04:43:09.174383Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Define a vectorized function to add the token to the oriinal string inside the dataset\nadd_token_vect = np.vectorize(\n    lambda x: f'{Tokens.START} ' + x.replace(',', f' {Tokens.COMMA}') + f' {Tokens.END}')\n\n# Apply the function to the 'generic_sentence' column of the DataFrame\ncorpus = add_token_vect(ds['generic_sentence'])","metadata":{"id":"kb5wPfO79wPB","execution":{"iopub.status.busy":"2024-06-12T04:43:09.176538Z","iopub.execute_input":"2024-06-12T04:43:09.177031Z","iopub.status.idle":"2024-06-12T04:43:14.965716Z","shell.execute_reply.started":"2024-06-12T04:43:09.177005Z","shell.execute_reply":"2024-06-12T04:43:14.964659Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Define a custom processing function to eliminate special characters","metadata":{"id":"a7tmpEy0cs9J"}},{"cell_type":"code","source":"def custom_preprocessing(text):\n    \"\"\"\n    this function is gonna remove every special character that is not an `<>,` from the original sentences.\n    text - text to be processed\n    \"\"\"\n    chars = string.punctuation\n    chars = chars.replace(\",\", \"\")\n    chars = chars.replace(\"<\", \"\")\n    chars = chars.replace(\">\", \"\")\n    # Remove punctuation\n    text = tf.strings.regex_replace(text, '[%s]' % re.escape(chars), '')\n    # Lowercase\n    text = tf.strings.lower(text)\n    # Remove punctuation\n    return text","metadata":{"id":"0bQvBDJ79wPB","execution":{"iopub.status.busy":"2024-06-12T04:43:14.974681Z","iopub.execute_input":"2024-06-12T04:43:14.974993Z","iopub.status.idle":"2024-06-12T04:43:14.980894Z","shell.execute_reply.started":"2024-06-12T04:43:14.974967Z","shell.execute_reply":"2024-06-12T04:43:14.979746Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Create the tokenizer, and use the processing function to tokenize the input","metadata":{"id":"rG6t3_3vc7yD"}},{"cell_type":"code","source":"tokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    standardize=custom_preprocessing,\n    output_sequence_length=SEQ_LEN,\n    output_mode='int',\n    pad_to_max_tokens=True,\n)\n\n#adapt the tokenizer to the text of the ds\ntokenizer.adapt(corpus)\n\nvocab = tokenizer.get_vocabulary()\n\n#visualize the first 10 tokens of the tokenizer\nprint(vocab[:10])","metadata":{"id":"m4NaVqz69wPB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"107ff4a5-2c07-48f7-caee-fe015239f480","execution":{"iopub.status.busy":"2024-06-12T04:43:14.982217Z","iopub.execute_input":"2024-06-12T04:43:14.983268Z","iopub.status.idle":"2024-06-12T04:43:17.872376Z","shell.execute_reply.started":"2024-06-12T04:43:14.983220Z","shell.execute_reply":"2024-06-12T04:43:17.871136Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"['', '[UNK]', '<start>', '<end>', 'the', 'of', 'and', '<comma>', 'is', 'to']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Defining the detokenizer","metadata":{"id":"gEmjfV-ed39I"}},{"cell_type":"code","source":"class TextDetokenizer:\n    def __init__(self, vectorize_layer):\n        self.vectorize_layer = vectorize_layer\n        vocab = self.vectorize_layer.get_vocabulary()\n        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n\n    def __detokenize_tokens(self, tokens):\n        def check_token(t):\n            if t == 2:\n                s = \"<start>\"\n            elif t == 3:\n                s = \"<end>\"\n            elif t == 7:\n                s = \"<comma>\"\n            else:\n                s = self.index_to_word.get(t, '[UNK]')\n            return s\n\n        return ' '.join([check_token(token) for token in tokens if token != 0])\n\n    def __call__(self, batch_tokens):\n        return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n\n#instantiate the detokenizer\ndetokenizer = TextDetokenizer(tokenizer)\n\n#tokenize the content of the whole dataset (corpus)\nsentences = tokenizer( corpus ).numpy()","metadata":{"id":"9jyqE4529wPC","execution":{"iopub.status.busy":"2024-06-12T04:43:17.873768Z","iopub.execute_input":"2024-06-12T04:43:17.874070Z","iopub.status.idle":"2024-06-12T04:43:20.539174Z","shell.execute_reply.started":"2024-06-12T04:43:17.874043Z","shell.execute_reply":"2024-06-12T04:43:20.538383Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"mask = np.sum( (sentences==1), axis=1) >= 1\noriginal_data = np.delete( sentences, mask , axis=0)\noriginal_data.shape","metadata":{"id":"bQahAe8a9wPD","outputId":"6a2d6588-62e0-4c0d-b0b1-9b698cd6beef","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-06-12T04:43:20.540732Z","iopub.execute_input":"2024-06-12T04:43:20.541096Z","iopub.status.idle":"2024-06-12T04:43:20.604383Z","shell.execute_reply.started":"2024-06-12T04:43:20.541061Z","shell.execute_reply":"2024-06-12T04:43:20.603402Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"(241194, 28)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Create a Data Generator for the Transformer Model\n\nFor the transformer architecture, we need a generator to provide the appropriate inputs and targets. The generator will output a tuple containing the input data and the target variable:\n\n- encoder_input: the scrambled input sentence.\n- decoder_input: the previous known sequence, starting with the <start> token.\n- decoder_output: the original sentence before scrambling, representing the expected output.","metadata":{"id":"oEDBf4rhcFLG"}},{"cell_type":"code","source":"class DataGenerator(keras.utils.PyDataset):\n    def __init__(self, data, batch_size=32, shuffle=True, seed=42, **kwargs):\n        super().__init__(**kwargs)\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.seed = seed\n        self.indexes = np.arange(len(self.data))\n\n    def __len__(self):\n        return math.ceil(len(self.data) / self.batch_size)\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        data_batch = np.array([self.data[k] for k in indexes])\n        result = np.copy(data_batch)\n\n        # shuffle the phrases inside the tags\n        for i in range(data_batch.shape[0]):\n            np.random.shuffle(data_batch[i, 1:data_batch[i].argmin() - 1])\n\n        encoder_input = data_batch\n        decoder_input = np.copy(result)\n        decoder_output = np.copy(result)\n        decoder_output = decoder_output[:, 1:]\n\n        decoder_output = np.pad(decoder_output, [[0, 0], [0, 1]], mode='constant')\n\n        return (encoder_input, decoder_input), decoder_output","metadata":{"id":"1ZXLkWB6od0R","execution":{"iopub.status.busy":"2024-06-12T04:43:20.605454Z","iopub.execute_input":"2024-06-12T04:43:20.605717Z","iopub.status.idle":"2024-06-12T04:43:20.616441Z","shell.execute_reply.started":"2024-06-12T04:43:20.605694Z","shell.execute_reply":"2024-06-12T04:43:20.615479Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"Shuffle the data","metadata":{"id":"fRTZyjSHi2xj"}},{"cell_type":"code","source":"# Make a random permutation of training and test set\nnp.random.seed(42)\n# Shuffle the all data\nshuffled_indices = np.random.permutation(len(original_data))\nshuffled_data = original_data[shuffled_indices]","metadata":{"id":"fNo_Jy3N8zHS","execution":{"iopub.status.busy":"2024-06-12T08:00:17.985127Z","iopub.execute_input":"2024-06-12T08:00:17.985880Z","iopub.status.idle":"2024-06-12T08:00:18.018749Z","shell.execute_reply.started":"2024-06-12T08:00:17.985845Z","shell.execute_reply":"2024-06-12T08:00:18.017807Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"Create the train and test generators","metadata":{"id":"387VUrEzjT5X"}},{"cell_type":"code","source":"#split the dataset\ntrain_generator = DataGenerator(shuffled_data[:220000], batch_size=BATCH_SIZE)\ntest_generator = DataGenerator(shuffled_data[225000:], batch_size=BATCH_SIZE)\n","metadata":{"id":"0-oFYQZI9wPE","execution":{"iopub.status.busy":"2024-06-12T04:43:20.660302Z","iopub.execute_input":"2024-06-12T04:43:20.660615Z","iopub.status.idle":"2024-06-12T04:43:20.665675Z","shell.execute_reply.started":"2024-06-12T04:43:20.660590Z","shell.execute_reply":"2024-06-12T04:43:20.664630Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Exploring how the generator outputs look like","metadata":{"id":"TPOtocSrjdR_"}},{"cell_type":"code","source":"x, y = train_generator.__getitem__(1)\n\nx_enc, x_dec = x\n\n#detokenize\nx_enc_inp_detok = detokenizer(x_enc)[0]\nx_dec_inp_detok = detokenizer(x_dec)[0]\ny_dec = detokenizer(y)[0]\n\n# print the sentences\nprint(\"Encoder Input: \",(x_enc_inp_detok))\nprint(\"Dencoder Input: \",(x_dec_inp_detok))\nprint(\"Target: \", y_dec)","metadata":{"id":"oyTcnf0VSuu3","outputId":"38185378-3fe6-45c4-9974-5a8c0a532841","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-06-12T04:43:20.670322Z","iopub.execute_input":"2024-06-12T04:43:20.670622Z","iopub.status.idle":"2024-06-12T04:43:20.694323Z","shell.execute_reply.started":"2024-06-12T04:43:20.670598Z","shell.execute_reply":"2024-06-12T04:43:20.693497Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Encoder Input:  <start> cholesterol type a steroid is lipid that a of special called is <end>\nDencoder Input:  <start> cholesterol is a special type of lipid that is called a steroid <end>\nTarget:  cholesterol is a special type of lipid that is called a steroid <end>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"Fo8MazCGBTv3"}},{"cell_type":"markdown","source":"Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n\n1.  look for the longest substring w between s and p\n2.  compute |w|/max(|s|,|p|)\n\nIf the match is exact, the score is 1.\n\nWhen computing the score, you should NOT consider the start and end tokens.\n\n","metadata":{"id":"G0NOkuO0CfPo"}},{"cell_type":"markdown","source":"The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric.","metadata":{"id":"a-aUrdlXDdVf"}},{"cell_type":"code","source":"from difflib import SequenceMatcher\n\n\ndef score(s, p):\n    match = SequenceMatcher(None, s, p).find_longest_match()\n    # print(match.size)\n    return (match.size/max(len(p), len(s)))","metadata":{"id":"ulpTRdrF_huh","execution":{"iopub.status.busy":"2024-06-12T04:43:20.695385Z","iopub.execute_input":"2024-06-12T04:43:20.696060Z","iopub.status.idle":"2024-06-12T04:43:20.700664Z","shell.execute_reply.started":"2024-06-12T04:43:20.696028Z","shell.execute_reply":"2024-06-12T04:43:20.699793Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"Let's do an example.","metadata":{"id":"RB2YfjXNExM-"}},{"cell_type":"code","source":"original = \"at first henry wanted to be friends with the king of france\"\ngenerated = \"henry wanted to be friends with king of france at the first\"\n\nprint(\"your score is \", score(original, generated))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h17C8bVjEwur","outputId":"4f7dccf7-4bd7-43ec-e63c-8a1e92cb28e4","execution":{"iopub.status.busy":"2024-06-12T04:43:20.701808Z","iopub.execute_input":"2024-06-12T04:43:20.702097Z","iopub.status.idle":"2024-06-12T04:43:20.711385Z","shell.execute_reply.started":"2024-06-12T04:43:20.702069Z","shell.execute_reply":"2024-06-12T04:43:20.710528Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"your score is  0.5423728813559322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The score must be computed as an average of at least 3K random examples taken form the test set.","metadata":{"id":"BET8GqBvFugR"}},{"cell_type":"markdown","source":"# What to deliver","metadata":{"id":"4fwo7xj4GBW1"}},{"cell_type":"markdown","source":"You are supposed to deliver a single notebook, suitably commented.\nThe notebook should describe a single model, although you may briefly discuss additional attempts you did.\n\nThe notebook should contain a full trace of the training.\nWeights should be made available on request.\n\nYou must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n\n# Good work!","metadata":{"id":"i6uITuxOGHfJ"}},{"cell_type":"markdown","source":"# Proposed model: transformer\n\nThe model is composed of two general components:\n\n- Encoder: Reads the input sequence (in this case, the scrambled words) and produces a fixed-dimensional vector representation.\n- Decoder: Generates the output sequence (original sentence) from the representation provided by the Encoder.\n\n\n## Why the transformer\nThe reason for choosing this arcitecture is its wide spread use in the industry applications, and its proven record of good results in NLP tasks. \n\n","metadata":{"id":"u4CSZD_SAcHm"}},{"cell_type":"markdown","source":"## Define the transformer model\n","metadata":{"id":"Syxo_zfnCgGX"}},{"cell_type":"markdown","source":"### Defining helper functions","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = ops.shape(inputs)[-1]\n        positions = ops.arange(0, length, 1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        if mask is None:\n            return None\n        else:\n            return ops.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"sequence_length\": self.sequence_length,\n                \"vocab_size\": self.vocab_size,\n                \"embed_dim\": self.embed_dim,\n            }\n        )\n        return config","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.712291Z","iopub.execute_input":"2024-06-12T04:43:20.712579Z","iopub.status.idle":"2024-06-12T04:43:20.726401Z","shell.execute_reply.started":"2024-06-12T04:43:20.712557Z","shell.execute_reply":"2024-06-12T04:43:20.725606Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"### Defining the encoder:","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.dropout_1 = layers.Dropout(dropout)\n        self.dropout_2 = layers.Dropout(dropout)\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n        else:\n            padding_mask = None\n\n        attention_output = self.attention(query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n        attention_output = self.dropout_1(attention_output)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        proj_output = self.dropout_2(proj_output)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n            \"dropout\": self.dropout,\n        })\n        return config","metadata":{"id":"9cev-dXcBZkg","execution":{"iopub.status.busy":"2024-06-12T04:43:20.727474Z","iopub.execute_input":"2024-06-12T04:43:20.728223Z","iopub.status.idle":"2024-06-12T04:43:20.739139Z","shell.execute_reply.started":"2024-06-12T04:43:20.728190Z","shell.execute_reply":"2024-06-12T04:43:20.738390Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### Defining the decoder:","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential([\n            layers.Dense(latent_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.dropout_1 = layers.Dropout(dropout)\n        self.dropout_2 = layers.Dropout(dropout)\n        self.dropout_3 = layers.Dropout(dropout)\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n            padding_mask = ops.minimum(padding_mask, causal_mask)\n        else:\n            padding_mask = None\n\n        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n        attention_output_1 = self.dropout_1(attention_output_1)\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(query=out_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n        attention_output_2 = self.dropout_2(attention_output_2)\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        proj_output = self.dropout_3(proj_output)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = ops.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = ops.arange(sequence_length)[:, None]\n        j = ops.arange(sequence_length)\n        mask = ops.cast(i >= j, dtype=\"int32\")\n        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = ops.concatenate(\n            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n            axis=0,\n        )\n        return ops.tile(mask, mult)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"latent_dim\": self.latent_dim,\n            \"num_heads\": self.num_heads,\n            \"dropout\": self.dropout,\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.740458Z","iopub.execute_input":"2024-06-12T04:43:20.740908Z","iopub.status.idle":"2024-06-12T04:43:20.757548Z","shell.execute_reply.started":"2024-06-12T04:43:20.740881Z","shell.execute_reply":"2024-06-12T04:43:20.756373Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Define a function that instantiates the transformer\ndef instantiate_transformer(seq_len, vocab_size, embedding_dim, latent_dim, num_heads, dropout_rate, num_layers):\n    \n    \"\"\"\n    Declares a sequence-to-sequence transformer, with multiple encoder and decoder layers.\n    \"\"\"\n    \n    # Encoder inputs\n    enc_inputs = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int64\", name=\"encoder_inputs\")\n    \n    # Embedding and positional encoding for encoder\n    enc_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(enc_inputs)\n    encoder_output = enc_embedding\n\n    # Encoder layers\n    encoder_layers = []\n    for i in range(num_layers):\n        encoder_layer = TransformerEncoder(embed_dim=embedding_dim, dense_dim=latent_dim, num_heads=num_heads, dropout=dropout_rate, name=f\"encoder_{i}\")\n        encoder_output = encoder_layer(encoder_output)\n        encoder_layers.append(encoder_layer)\n\n    # Decoder inputs\n    dec_inputs = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int64\", name=\"decoder_inputs\")\n    \n    # Embedding and positional encoding for decoder\n    dec_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(dec_inputs)\n    decoder_output = dec_embedding\n\n    # Decoder layers\n    decoder_layers = []\n    for i in range(num_layers):\n        decoder_layer = TransformerDecoder(embed_dim=embedding_dim, latent_dim=latent_dim, num_heads=num_heads, dropout=dropout_rate, name=f\"decoder_{i}\")\n        decoder_output = decoder_layer(decoder_output, encoder_output)\n        decoder_layers.append(decoder_layer)\n\n    # Output layer\n    outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(decoder_output)\n\n    transformer = tf.keras.Model(inputs=[enc_inputs, dec_inputs], outputs=outputs, name=\"transformer_model\")\n    transformer.summary()\n    return transformer","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.758577Z","iopub.execute_input":"2024-06-12T04:43:20.759981Z","iopub.status.idle":"2024-06-12T04:43:20.774130Z","shell.execute_reply.started":"2024-06-12T04:43:20.759944Z","shell.execute_reply":"2024-06-12T04:43:20.773245Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"# Training the model\n\nTo train the transformer model, we define custom loss and scheduling functions, configure the model with hyperparameters, and implement an early stopping mechanism to save the best model.\n\n- Custom Loss Function: Computes the loss using SparseCategoricalCrossentropy and adds a penalty for incorrect word positions.\n- Custom Accuracy Function: Measures the accuracy by counting correctly positioned words in the decoded sentence.\n- Learning Rate Scheduler: The CustomSchedule custom learning rate scheduler is inspired by the \"Attention is All You Need\" paper, adjusting the learning rate during training. The optimizer also uses parameters from the same paper.","metadata":{"id":"tlGUPebbC0Io"}},{"cell_type":"code","source":"def loss_func(target, predictions):\n    \n    \"\"\"\n    Calculates the mean sparse categorical cross-entropy loss for each position in the target sequence. \n    Applies a mask to ignore invalid positions (zeros) in the target. \n    This ensures that the loss is computed only over valid (non-zero) positions.\n    \"\"\"\n    \n    valid_positions = tf.not_equal(target, 0)\n\n    raw_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(target, predictions)\n\n    valid_positions = tf.cast(valid_positions, dtype=raw_loss.dtype)\n\n    masked_loss = raw_loss * valid_positions\n\n    mean_loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(valid_positions)\n    \n    return mean_loss","metadata":{"id":"GzJFIyKnamPX","execution":{"iopub.status.busy":"2024-06-12T04:43:20.775245Z","iopub.execute_input":"2024-06-12T04:43:20.775621Z","iopub.status.idle":"2024-06-12T04:43:20.788697Z","shell.execute_reply.started":"2024-06-12T04:43:20.775563Z","shell.execute_reply":"2024-06-12T04:43:20.787813Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#define a decoding function\nvocab = tokenizer.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = 27\n\ndef decode_sequence(input_sentence):\n    \n    \"\"\"\n    Generates a sequence of tokens from an input sentence using a trained model. \n    Predicts the next token iteratively and appends it to the decoded sentence.\n    It stops when the end token is reached or the maximum sentence length is exceeded.\n    \"\"\"\n    \n    tokenized_input_sentence = tokenizer([input_sentence])\n    decoded_sentence = Tokens.START\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = tokenizer([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = ops.convert_to_numpy(\n            ops.argmax(predictions[0, i, :])\n        ).item(0)\n\n        sampled_token = spa_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == Tokens.END:\n            break\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.789899Z","iopub.execute_input":"2024-06-12T04:43:20.790208Z","iopub.status.idle":"2024-06-12T04:43:20.847055Z","shell.execute_reply.started":"2024-06-12T04:43:20.790183Z","shell.execute_reply":"2024-06-12T04:43:20.846376Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"def clean_sentence(sentence):\n    \"\"\"\n    Eliminates the \"start\", \"end\" and \"comma\" tokens from a sentence.\n    \"\"\"\n    clean_sentence = sentence.replace(Tokens.START, \"\").replace(Tokens.END, \"\").replace(Tokens.COMMA, \",\").strip()\n    return clean_sentence","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.848160Z","iopub.execute_input":"2024-06-12T04:43:20.848512Z","iopub.status.idle":"2024-06-12T04:43:20.853391Z","shell.execute_reply.started":"2024-06-12T04:43:20.848481Z","shell.execute_reply":"2024-06-12T04:43:20.852544Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n\nsample_indices = np.random.choice(np.arange(230000, 235000), size=3840, replace=False)\ndata_gen = DataGenerator(shuffled_data[sample_indices], batch_size=BATCH_SIZE)\n\ndef evaluate_model_val(transformer, vocab, batch_size, seq_len, score_func):\n    \n    \"\"\"\n    Evaluates a transformer model on a validation dataset generated by data_gen. \n    Iterates through batches, performs predictions, and computes a score for each predicted sequence.\n    The metric used is the proposed metric by the requirements of the assignment.\n    Returns the score for the current version of the model.\n    \n    INPUTS:\n    \n    transfomer - the trained model\n    vocab - the vocabulary list obtained from the tokenizer\n    batch_size - numper of samples per batch\n    seq_len - maximum length of a sequence\n    score_func - metric proposed by the assignment.\n    \n    OUTPUT:\n    \n    avg_score - the score computed over all the batches.\n    \n    \"\"\"\n    \n    scores = []\n\n    for i in tqdm(range(len(data_gen)), desc=\"Evaluating batches\"):\n        input_data, _ = data_gen[i]\n        enc_input, dec_input = input_data\n\n        if enc_input.shape[0] < batch_size:\n            continue\n\n        output_ta = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        start_token = np.array([vocab.index(Tokens.START)], ndmin=1)\n        output_ta = output_ta.write(0, tf.tile(start_token, [batch_size]))\n\n        for j in range(seq_len):\n            current_output = tf.transpose(output_ta.stack())\n            pred = transformer.predict([enc_input, current_output], verbose=0)\n\n            last_prediction = pred[:, -1:, :]\n            next_token = tf.argmax(last_prediction, axis=-1)\n            output_ta = output_ta.write(j + 1, next_token[:, 0])\n\n            if tf.reduce_all(tf.reduce_any(tf.equal(next_token, vocab.index(Tokens.END)), axis=-1)):\n                break\n\n        final_output = tf.transpose(output_ta.stack()).numpy()\n        predicted_sequences = detokenizer(final_output)\n        actual_sequences = detokenizer(dec_input)\n\n        for predicted, true_sentence in zip(predicted_sequences, actual_sequences):\n            cleaned_pred = clean_sentence(predicted)\n            clean_true_sentence = clean_sentence(true_sentence)\n            scores.append(score_func(cleaned_pred, clean_true_sentence))\n\n    avg_score = np.mean(scores)\n\n    print(\"Average score: \", avg_score)\n\n    return avg_score\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.854958Z","iopub.execute_input":"2024-06-12T04:43:20.855331Z","iopub.status.idle":"2024-06-12T04:43:20.870096Z","shell.execute_reply.started":"2024-06-12T04:43:20.855300Z","shell.execute_reply":"2024-06-12T04:43:20.869292Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"\n    Implements the learning rate schedule as described in the \"Attention Is All You Need\" paper. \n    \"\"\"\n    \n    def __init__(self, emb_dim, warmup_steps=4000):\n        super().__init__()\n        self.emb_dim = emb_dim\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        argument1 = step * tf.cast((self.warmup_steps ** -1.5), dtype=tf.float32)\n        argument2 = tf.math.rsqrt(step)\n\n        return tf.math.rsqrt(tf.cast(self.emb_dim, dtype=tf.float32)) * tf.math.minimum(argument1, argument2)\n\n    def get_config(self):\n        return {'emb_dim': self.emb_dim,'warmup_steps': self.warmup_steps}","metadata":{"execution":{"iopub.status.busy":"2024-06-12T04:43:20.871159Z","iopub.execute_input":"2024-06-12T04:43:20.871472Z","iopub.status.idle":"2024-06-12T04:43:20.883230Z","shell.execute_reply.started":"2024-06-12T04:43:20.871447Z","shell.execute_reply":"2024-06-12T04:43:20.882391Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{"id":"i5dGaCGzGbX2"}},{"cell_type":"code","source":"#Hyperpameters and parameters of the model\nEMBEDDING_DIM = 128\nLATENT_DIM = 600\nNUM_HEADS = 14\nNUM_LAYERS = 5\nDROPOUT = 0.15\nEPOCHS = 35\nPATIENCE = 5\n\n\n#Initialize the transformer\ntransformer = instantiate_transformer(\n    seq_len=SEQ_LEN, \n    vocab_size=VOCAB_SIZE, \n    embedding_dim=EMBEDDING_DIM, \n    latent_dim=LATENT_DIM, \n    num_heads=NUM_HEADS, \n    dropout_rate=DROPOUT, \n    num_layers=NUM_LAYERS\n)\n\n\n# Defining the optimizer and compiling the model\nlearning_rate = CustomSchedule(emb_dim=EMBEDDING_DIM, warmup_steps=4000)\noptimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, weight_decay = 0.005)\ntransformer.compile(optimizer=optimizer, loss=loss_func, metrics=[\"accuracy\"])\n\n\n# Training the model\n# Contains an early stopping mechanism that stops the training is the score does not improve after a specified number of epochs (PATIENCE).\n# The score is compared every 4 epochs. this was done to speed up the training time, as previous experiments determined the necessity of a high number of epoch for the model to converge.\n\nbest_transformer = transformer\nbest_score = -1\npatience_counter = 0\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch: {epoch}\")\n    transformer.fit(train_generator)\n\n    if epoch % 4 == 0 and epoch != 0:\n        current_score = evaluate_model_val(transformer, vocab, BATCH_SIZE, SEQ_LEN, score)\n        \n        if current_score > best_score:\n            best_transformer = transformer\n            best_score = current_score\n            patience_counter = 0\n            transformer.save('model.weights.h5')\n            print(f\"Model Updated and Saved. Current score: {best_score}\")\n        else:\n            patience_counter += 1\n        \n        if patience_counter > PATIENCE:\n            print(\"Early stopping due to no improvement in score.\")\n            break\n            \n# Save the final best model\nbest_transformer.save('model.weights.h5')","metadata":{"id":"637aCsQ2DtE5","outputId":"f0a1b417-1ac8-41a0-f8eb-3de0fbf97b93","colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2024-06-12T05:00:34.561284Z","iopub.execute_input":"2024-06-12T05:00:34.561705Z","iopub.status.idle":"2024-06-12T07:50:14.383322Z","shell.execute_reply.started":"2024-06-12T05:00:34.561675Z","shell.execute_reply":"2024-06-12T07:50:14.381761Z"},"trusted":true},"execution_count":95,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer_model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,280,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,077,848\u001b[0m │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,077,848\u001b[0m │ encoder_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,077,848\u001b[0m │ encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,077,848\u001b[0m │ encoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_13        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,280,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,077,848\u001b[0m │ encoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,001,112\u001b[0m │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,001,112\u001b[0m │ decoder_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,001,112\u001b[0m │ decoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,001,112\u001b[0m │ decoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,001,112\u001b[0m │ decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_142 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m1,290,000\u001b[0m │ decoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,848</span> │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,848</span> │ encoder_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,848</span> │ encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,848</span> │ encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_13        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,077,848</span> │ encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,112</span> │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,112</span> │ decoder_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,112</span> │ decoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,112</span> │ decoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,112</span> │ decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │ decoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,244,800\u001b[0m (73.41 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,244,800</span> (73.41 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,244,800\u001b[0m (73.41 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,244,800</span> (73.41 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718168530.020025     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m275/860\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:36\u001b[0m 575ms/step - accuracy: 0.0232 - loss: 8.9185","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718168687.837173     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 435ms/step - accuracy: 0.0564 - loss: 7.9669\nEpoch: 1\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 369ms/step - accuracy: 0.2265 - loss: 3.7855\nEpoch: 2\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 370ms/step - accuracy: 0.3122 - loss: 1.9657\nEpoch: 3\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 370ms/step - accuracy: 0.3433 - loss: 1.2909\nEpoch: 4\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 369ms/step - accuracy: 0.3586 - loss: 1.0295\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches:   0%|          | 0/15 [00:00<?, ?it/s]W0000 00:00:1718170180.097195     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170184.428724     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170187.018038     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170189.629851     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170192.282157     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170194.984850     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170197.880596     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170200.799736     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170203.493764     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170206.421020     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170209.193282     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170212.062314     172 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170214.916509     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170217.710980     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170220.456650     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170223.357862     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170226.372275     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170229.216484     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170232.036072     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170234.865200     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170237.786829     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nEvaluating batches:  20%|██        | 3/15 [01:14<03:45, 18.80s/it]W0000 00:00:1718170259.596810     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718170262.514025     172 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nEvaluating batches: 100%|██████████| 15/15 [02:37<00:00, 10.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.44471857626852523\nModel Updated and Saved. Current score: 0.44471857626852523\nEpoch: 5\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 368ms/step - accuracy: 0.3724 - loss: 0.8531\nEpoch: 6\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.3859 - loss: 0.7029\nEpoch: 7\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 368ms/step - accuracy: 0.3959 - loss: 0.6013\nEpoch: 8\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 368ms/step - accuracy: 0.4048 - loss: 0.5185\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches:  13%|█▎        | 2/15 [00:13<01:27,  6.74s/it]W0000 00:00:1718171629.104779     172 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718171632.143426     170 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718171635.714270     173 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nEvaluating batches: 100%|██████████| 15/15 [01:51<00:00,  7.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.4921824103470553\nModel Updated and Saved. Current score: 0.4921824103470553\nEpoch: 9\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4119 - loss: 0.4582\nEpoch: 10\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4171 - loss: 0.4113\nEpoch: 11\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4230 - loss: 0.3654\nEpoch: 12\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4273 - loss: 0.3291\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches: 100%|██████████| 15/15 [01:35<00:00,  6.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5153522297199582\nModel Updated and Saved. Current score: 0.5153522297199582\nEpoch: 13\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4319 - loss: 0.2991\nEpoch: 14\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4349 - loss: 0.2722\nEpoch: 15\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4379 - loss: 0.2515\nEpoch: 16\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 368ms/step - accuracy: 0.4408 - loss: 0.2299\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches:  13%|█▎        | 2/15 [00:12<01:23,  6.46s/it]W0000 00:00:1718174369.893280     172 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718174373.273662     171 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nEvaluating batches: 100%|██████████| 15/15 [01:53<00:00,  7.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5278042561444067\nModel Updated and Saved. Current score: 0.5278042561444067\nEpoch: 17\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4434 - loss: 0.2123\nEpoch: 18\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 368ms/step - accuracy: 0.4459 - loss: 0.1974\nEpoch: 19\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4479 - loss: 0.1830\nEpoch: 20\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4495 - loss: 0.1681\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches: 100%|██████████| 15/15 [01:41<00:00,  6.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5287415463729048\nModel Updated and Saved. Current score: 0.5287415463729048\nEpoch: 21\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 368ms/step - accuracy: 0.4512 - loss: 0.1584\nEpoch: 22\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4531 - loss: 0.1483\nEpoch: 23\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 367ms/step - accuracy: 0.4544 - loss: 0.1383\nEpoch: 24\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 366ms/step - accuracy: 0.4561 - loss: 0.1293\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches: 100%|██████████| 15/15 [01:44<00:00,  6.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5319024272395863\nModel Updated and Saved. Current score: 0.5319024272395863\nEpoch: 25\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4568 - loss: 0.1231\nEpoch: 26\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 367ms/step - accuracy: 0.4579 - loss: 0.1161\nEpoch: 27\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4587 - loss: 0.1135\nEpoch: 28\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 367ms/step - accuracy: 0.4599 - loss: 0.1046\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches: 100%|██████████| 15/15 [01:37<00:00,  6.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5391739215830544\nModel Updated and Saved. Current score: 0.5391739215830544\nEpoch: 29\n\u001b[1m152/860\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:22\u001b[0m 370ms/step - accuracy: 0.4617 - loss: 0.0915","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[95], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m         current_score \u001b[38;5;241m=\u001b[39m evaluate_model_val(transformer, vocab, BATCH_SIZE, SEQ_LEN, score)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Testing the model\n\nThe final model will be evaluated in this section.\nThe metric used is the metric proposed by the assignment.\n\nWe will use the evaluation function defined for the validation to test the model. In this instance, we will use a generator that iterates over the remaining data points(the unseen data in the test set).","metadata":{"id":"u2Z9Rpj1JvMw"}},{"cell_type":"code","source":"from tqdm import tqdm\n\nprint(f\"The test set has {shuffled_data.shape[0] - 235000} samples.\")\ndata_gen = DataGenerator(shuffled_data[235000:], batch_size=BATCH_SIZE)\nevaluate_model_val(best_transformer, vocab, BATCH_SIZE, SEQ_LEN, score)","metadata":{"id":"zwvKMh4UamPY","execution":{"iopub.status.busy":"2024-06-12T08:51:56.996798Z","iopub.execute_input":"2024-06-12T08:51:56.997606Z","iopub.status.idle":"2024-06-12T08:54:48.214186Z","shell.execute_reply.started":"2024-06-12T08:51:56.997569Z","shell.execute_reply":"2024-06-12T08:54:48.213196Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"The test set has 6194 samples\n","output_type":"stream"},{"name":"stderr","text":"Evaluating batches: 100%|██████████| 25/25 [02:51<00:00,  6.85s/it]","output_type":"stream"},{"name":"stdout","text":"Average score:  0.5376171068203628\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"0.5376171068203628"},"metadata":{}}]},{"cell_type":"markdown","source":"We can observe that the model has a performance of 53.7% on the metric provided by the assignment.\n\nIn the end, we will save the model weights, in order to be able to reproduce the test results if needed.","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:11:52.497226Z","iopub.execute_input":"2024-06-12T08:11:52.497662Z","iopub.status.idle":"2024-06-12T08:11:52.504255Z","shell.execute_reply.started":"2024-06-12T08:11:52.497634Z","shell.execute_reply":"2024-06-12T08:11:52.502984Z"}}},{"cell_type":"code","source":"best_transformer.save('best_transformer.weights.h5')\ntransformer.save('transformer.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:10.193541Z","iopub.execute_input":"2024-06-12T08:12:10.194165Z","iopub.status.idle":"2024-06-12T08:12:11.830495Z","shell.execute_reply.started":"2024-06-12T08:12:10.194133Z","shell.execute_reply":"2024-06-12T08:12:11.829479Z"},"trusted":true},"execution_count":101,"outputs":[]}]}